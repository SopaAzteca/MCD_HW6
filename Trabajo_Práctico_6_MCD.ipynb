{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GItxU96mTk16"
   },
   "source": [
    "# Matemática para Ciencia de los Datos\n",
    "# Trabajo Práctico 6\n",
    "\n",
    "Profesor: Juan Luis Crespo Mariño (basado en trabajo previo de Luis Alexánder Calvo Valverde)\n",
    "\n",
    "Instituto Tecnológico de Costa Rica,\n",
    "\n",
    "Programa Ciencia de Datos\n",
    "\n",
    "---\n",
    "\n",
    "Fecha de entrega: 26 de Noviembre del 2023, a más tardar a las 6:00 pm.\n",
    "\n",
    "Medio de entrega: Por medio del TEC-Digital.\n",
    "\n",
    "Entregables: Un archivo jupyter ( .IPYNB ).\n",
    "\n",
    "Estudiante:\n",
    "1. Luis Felipe Quesada Miranda\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ce5WVgoNSSAf"
   },
   "source": [
    "## Ejercicio 1 (50 puntos)\n",
    "\n",
    "\n",
    "\n",
    "El algoritmo del descenso de gradiente sigue la idea de modificar el punto óptimo estimado de forma iterativa. Para una función en una\n",
    "variable $f\\left(x\\right)$, la estimación del punto óptimo en una iteración $i+1$ está dada por:\n",
    "\n",
    "\\begin{equation}\n",
    "x\\left(t+1\\right)=x\\left(t\\right)+\\alpha f'\\left(x\\left(t\\right)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "donde el coeficiente $\\alpha$ determina el *grado de confianza o velocidad* con la que el proceso de optimización iterativa sigue\n",
    "la dirección de la derivada. Para la optimización de una función multivariable $f\\left(\\overrightarrow{x}\\left(t\\right)\\right)$ con $\\overrightarrow{x}\\in\\mathbb{R}^{n}$, la posición óptima se estima usando el vector gradiente:\n",
    "\n",
    "\\begin{equation}\n",
    "\\overrightarrow{x}\\left(t+1\\right)=\\overrightarrow{x}\\left(t\\right)+\\alpha\\nabla_{\\overrightarrow{x}}f\\left(\\overrightarrow{x}\\left(t\\right)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Para la función:\n",
    "\n",
    "\\begin{equation}\n",
    "f\\left(\\overrightarrow{x}\\right)=x^{2}-y^{2},\n",
    "\\end{equation}\n",
    "\n",
    "Implemente la función en python denominada:\n",
    "\n",
    "$$funcion\\_SGD \\left(tasa\\_aprendizaje, iteraciones, xy, tolerancia\\right)$$\n",
    "\n",
    "donde los parámetros corresponden a:\n",
    "\n",
    "* tasa_aprendizaje: es el $\\alpha$\n",
    "* iteraciones: es el máximo número de iteraciones a ejecutar\n",
    "* xy: es el vector con los dos valores iniciales [x,y]\n",
    "* tolerancia: es el valor mínimo para un cambio entre iteración. Si la función de costo no mejora en al menos \"tolerancia\", sale del ciclo de iteración.\n",
    "\n",
    "**Nota:**\n",
    "1. Para iniciar la implementación puede utilizar el código en el cuaderno \"070_1_LACV_Optimizacion\".\n",
    "1. Cada iteración le generará un vector con dos valores ($\\overrightarrow{x}\\left(t+1\\right)$), por lo que para saber el valor de la función de pérdida en ese punto, evalúelo en la función inicial ($x^{2}-y^{2}$) para saber si aumentó o disminuyó.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvpiG31kSSAg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRBmuhcjSSAh"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GoSa-9MSSAh"
   },
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "Para la función  $f_{1}\\left(x_{1},x_{2}\\right)=x_{1}^4 + x_{2}^4$\n",
    "\n",
    "Realice lo siguiente:\n",
    "\n",
    "1. En una celda de texto:\n",
    "\n",
    " - Calcule el vector gradiente. **(15 puntos)**\n",
    "\n",
    " - Calcule la matriz Hessiana. **(15 puntos)**\n",
    "\n",
    "2. Para el resultado obtenido en el punto anterior: **(20 puntos)**\n",
    "  - Evalúela en el punto $x_{1},x_{2}\\in\\left[4,4\\right]$.\n",
    "  - Luego aplique el criterio de la segunda derivada parcial ¿qué conclusiones saca para ese punto?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$\n",
    "\n",
    "𝑓_{1}(𝑥_{1},𝑥_{2}) = 𝑥^{4}_{1} + 𝑥^{4}_{2}\\\\\n",
    "\\textrm{Calculamos todas las derivadas parciales:}\\\\\n",
    "\n",
    "\\frac{∂𝑓_{1}}{∂𝑥_{1}} =  4𝑥^{3}_{1} \\\\\n",
    "\\frac{∂𝑓_{1}}{∂𝑥_2} = 4𝑥^{3}_{2}\\\\\n",
    "\n",
    "\\textrm{Segundo orden:}\\\\\n",
    "\n",
    "\\frac{∂^{2}f_{1}}{∂x^{2}_{1}} =  12x^{2}_{1}\\\\\n",
    "\\frac {∂^{2}f_{1}}{∂x^{2}_{2}} =  12x^{2}_{2}\\\\\n",
    "\n",
    "\\frac {∂^{2}f_{1}}{∂x_{1}∂x_{2}} =  0 \\\\\n",
    "\n",
    "\\frac {∂^{2}f_{1}}{∂x_{2}∂x_{1}} =  0 \\\\\n",
    "\n",
    "\\textrm{Creamos la matriz Hessiana:}\\\\\n",
    "\n",
    "𝐻(𝑓_{1}) = \\begin{bmatrix} 12𝑥^{2}_{1} & 0 \\\\\\\\ 0 & 12x^{2}_{2} \\end{bmatrix}\\\\\n",
    "\n",
    "\\textrm{Evaluamos en el punto (4,4):}\\\\\n",
    "\n",
    "𝐻(𝑓_{1}|_{(4,4)}) =\\begin{bmatrix} 12(4)^{2} & 0 \\\\\\\\ 0 & 12(4)^{2} \\end{bmatrix}\\\\\n",
    "\n",
    "\n",
    "𝐻(𝑓_{1}|_{(4,4)}) =\\begin{bmatrix} 192 & 0 \\\\\\\\ 0 & 192 \\end{bmatrix}\\\\\n",
    "\n",
    "\\textrm{Calculamos el determinante de la matriz Hessiana:}\\\\\n",
    "\n",
    "\\textrm{ det(H) = 192*192 - 0*0 =  36,864 }\\\\\n",
    "\n",
    "\n",
    "\n",
    "\\textrm{Ya que det(H) > 0 y }  \\frac {∂^{2}f_{1}}{∂x^{2}_{1}} = 192 \\\\ \\textrm {el punto (4,4) es un minimo local}\n",
    "\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
