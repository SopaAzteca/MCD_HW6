{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GItxU96mTk16"
   },
   "source": [
    "# Matem谩tica para Ciencia de los Datos\n",
    "# Trabajo Pr谩ctico 6\n",
    "\n",
    "Profesor: Juan Luis Crespo Mari帽o (basado en trabajo previo de Luis Alex谩nder Calvo Valverde)\n",
    "\n",
    "Instituto Tecnol贸gico de Costa Rica,\n",
    "\n",
    "Programa Ciencia de Datos\n",
    "\n",
    "---\n",
    "\n",
    "Fecha de entrega: 26 de Noviembre del 2023, a m谩s tardar a las 6:00 pm.\n",
    "\n",
    "Medio de entrega: Por medio del TEC-Digital.\n",
    "\n",
    "Entregables: Un archivo jupyter ( .IPYNB ).\n",
    "\n",
    "Estudiante:\n",
    "1. Luis Felipe Quesada Miranda\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ce5WVgoNSSAf"
   },
   "source": [
    "## Ejercicio 1 (50 puntos)\n",
    "\n",
    "\n",
    "\n",
    "El algoritmo del descenso de gradiente sigue la idea de modificar el punto 贸ptimo estimado de forma iterativa. Para una funci贸n en una\n",
    "variable $f\\left(x\\right)$, la estimaci贸n del punto 贸ptimo en una iteraci贸n $i+1$ est谩 dada por:\n",
    "\n",
    "\\begin{equation}\n",
    "x\\left(t+1\\right)=x\\left(t\\right)+\\alpha f'\\left(x\\left(t\\right)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "donde el coeficiente $\\alpha$ determina el *grado de confianza o velocidad* con la que el proceso de optimizaci贸n iterativa sigue\n",
    "la direcci贸n de la derivada. Para la optimizaci贸n de una funci贸n multivariable $f\\left(\\overrightarrow{x}\\left(t\\right)\\right)$ con $\\overrightarrow{x}\\in\\mathbb{R}^{n}$, la posici贸n 贸ptima se estima usando el vector gradiente:\n",
    "\n",
    "\\begin{equation}\n",
    "\\overrightarrow{x}\\left(t+1\\right)=\\overrightarrow{x}\\left(t\\right)+\\alpha\\nabla_{\\overrightarrow{x}}f\\left(\\overrightarrow{x}\\left(t\\right)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Para la funci贸n:\n",
    "\n",
    "\\begin{equation}\n",
    "f\\left(\\overrightarrow{x}\\right)=x^{2}-y^{2},\n",
    "\\end{equation}\n",
    "\n",
    "Implemente la funci贸n en python denominada:\n",
    "\n",
    "$$funcion\\_SGD \\left(tasa\\_aprendizaje, iteraciones, xy, tolerancia\\right)$$\n",
    "\n",
    "donde los par谩metros corresponden a:\n",
    "\n",
    "* tasa_aprendizaje: es el $\\alpha$\n",
    "* iteraciones: es el m谩ximo n煤mero de iteraciones a ejecutar\n",
    "* xy: es el vector con los dos valores iniciales [x,y]\n",
    "* tolerancia: es el valor m铆nimo para un cambio entre iteraci贸n. Si la funci贸n de costo no mejora en al menos \"tolerancia\", sale del ciclo de iteraci贸n.\n",
    "\n",
    "**Nota:**\n",
    "1. Para iniciar la implementaci贸n puede utilizar el c贸digo en el cuaderno \"070_1_LACV_Optimizacion\".\n",
    "1. Cada iteraci贸n le generar谩 un vector con dos valores ($\\overrightarrow{x}\\left(t+1\\right)$), por lo que para saber el valor de la funci贸n de p茅rdida en ese punto, eval煤elo en la funci贸n inicial ($x^{2}-y^{2}$) para saber si aument贸 o disminuy贸.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvpiG31kSSAg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRBmuhcjSSAh"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GoSa-9MSSAh"
   },
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "Para la funci贸n  $f_{1}\\left(x_{1},x_{2}\\right)=x_{1}^4 + x_{2}^4$\n",
    "\n",
    "Realice lo siguiente:\n",
    "\n",
    "1. En una celda de texto:\n",
    "\n",
    " - Calcule el vector gradiente. **(15 puntos)**\n",
    "\n",
    " - Calcule la matriz Hessiana. **(15 puntos)**\n",
    "\n",
    "2. Para el resultado obtenido en el punto anterior: **(20 puntos)**\n",
    "  - Eval煤ela en el punto $x_{1},x_{2}\\in\\left[4,4\\right]$.\n",
    "  - Luego aplique el criterio de la segunda derivada parcial 驴qu茅 conclusiones saca para ese punto?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$\n",
    "\n",
    "_{1}(_{1},_{2}) = ^{4}_{1} + ^{4}_{2}\\\\\n",
    "\\textrm{Calculamos todas las derivadas parciales:}\\\\\n",
    "\n",
    "\\frac{_{1}}{_{1}} =  4^{3}_{1} \\\\\n",
    "\\frac{_{1}}{_2} = 4^{3}_{2}\\\\\n",
    "\n",
    "\\textrm{Segundo orden:}\\\\\n",
    "\n",
    "\\frac{^{2}f_{1}}{x^{2}_{1}} =  12x^{2}_{1}\\\\\n",
    "\\frac {^{2}f_{1}}{x^{2}_{2}} =  12x^{2}_{2}\\\\\n",
    "\n",
    "\\frac {^{2}f_{1}}{x_{1}x_{2}} =  0 \\\\\n",
    "\n",
    "\\frac {^{2}f_{1}}{x_{2}x_{1}} =  0 \\\\\n",
    "\n",
    "\\textrm{Creamos la matriz Hessiana:}\\\\\n",
    "\n",
    "(_{1}) = \\begin{bmatrix} 12^{2}_{1} & 0 \\\\\\\\ 0 & 12x^{2}_{2} \\end{bmatrix}\\\\\n",
    "\n",
    "\\textrm{Evaluamos en el punto (4,4):}\\\\\n",
    "\n",
    "(_{1}|_{(4,4)}) =\\begin{bmatrix} 12(4)^{2} & 0 \\\\\\\\ 0 & 12(4)^{2} \\end{bmatrix}\\\\\n",
    "\n",
    "\n",
    "(_{1}|_{(4,4)}) =\\begin{bmatrix} 192 & 0 \\\\\\\\ 0 & 192 \\end{bmatrix}\\\\\n",
    "\n",
    "\\textrm{Calculamos el determinante de la matriz Hessiana:}\\\\\n",
    "\n",
    "\\textrm{ det(H) = 192*192 - 0*0 =  36,864 }\\\\\n",
    "\n",
    "\n",
    "\n",
    "\\textrm{Ya que det(H) > 0 y }  \\frac {^{2}f_{1}}{x^{2}_{1}} = 192 \\\\ \\textrm {el punto (4,4) es un minimo local}\n",
    "\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
